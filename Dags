## Airflow DAG (Orchestration)
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
import pandas as pd
import os

def extract():
    file_path = "data/FCT_RESTAURANT_ORDERS.csv"
    df = pd.read_csv(file_path)
    df.to_csv("data/processed_orders.csv", index=False)
    print("Extracted data from FCT_RESTAURANT_ORDERS.csv")

def transform():
    df = pd.read_csv("data/processed_orders.csv")
    df["order_date"] = pd.to_datetime(df["order_date"])
    df["total_amount"] = df["quantity"] * df["price"]
    df.to_csv("data/transformed_orders.csv", index=False)
    print("Transformed data and calculated total_amount")

def load():
    print("Loading transformed data into Snowflake")

def notify():
    print("Pipeline completed successfully")

default_args = {"start_date": datetime(2024, 3, 12), "catchup": False}

dag = DAG("snowflake_pipeline", default_args=default_args, schedule_interval="@daily")

extract_task = PythonOperator(task_id="extract", python_callable=extract, dag=dag)
transform_task = PythonOperator(task_id="transform", python_callable=transform, dag=dag)
load_task = PythonOperator(task_id="load", python_callable=load, dag=dag)
notify_task = PythonOperator(task_id="notify", python_callable=notify, dag=dag)

extract_task >> transform_task >> load_task >> notify_task
```
